<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Topological Learning &#8212; infotopo 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to Use InfoTopo" href="tutorial.html" />
    <link rel="prev" title="InfoTopo: Topological Information Data Analysis. Deep statistical unsupervised and supervised learning." href="index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          infotopo</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="#">Method</a></li>
                <li><a href="tutorial.html">Tutorial</a></li>
                <li><a href="install.html">Install</a></li>
                <li><a href="api.html">API</a></li>
                <li><a href="auto_examples/index.html">Examples</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption"><span class="caption-text">User Guide / Tutorial:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Topological Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#topological-learning-principles">Topological Learning principles</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#information-complexes">Information Complexes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#poincare-shannon-machine">Poincaré-Shannon Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#unsupervised-topological-learning">Unsupervised topological learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#causality-challenge-dataset">Causality challenge dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Information Complexes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#digits-dataset">Digits Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-computational-complexity">Adaptive computational complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id3">Unsupervised topological learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">How to Use InfoTopo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tutorial.html#iris-data">Iris data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#iris-dataset">Iris dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#entropy">Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#mutual-information">Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#information-networks">Information Networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tutorial.html#diabetes-data">Diabetes data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#diabetes-dataset">Diabetes dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#id4">Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#total-correlation">Total correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#id5">Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#id6">Information Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#mean-information-path">Mean Information path</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#conditional-transfer-informations">Conditional (transfer) Informations</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#entropy-vs-energy">Entropy Vs. Energy</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html#information-distance">Information distance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#standard-installation">Standard installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-the-most-up-to-date-version">Install the most up-to-date version</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/method.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="topological-learning">
<h1>Topological Learning<a class="headerlink" href="#topological-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="topological-learning-principles">
<h2>Topological Learning principles<a class="headerlink" href="#topological-learning-principles" title="Permalink to this headline">¶</a></h2>
<div class="section" id="information-complexes">
<h3>Information Complexes<a class="headerlink" href="#information-complexes" title="Permalink to this headline">¶</a></h3>
<p>The presentation of the basic methods and principles we made so far mostly relied on basic information lattice decomposition and simplex structure.
In what follows, we will go one stepp further by introducing to simplicial complexes of information which can display much richer structures. This will be the
occasion to study more in depth information paths, the analog of homotopical paths in information theory. We will consider subcomplex of this simplicial structure,
invocating the fact that any simplicial complex can be realized as a subcomplex of a simplex (<a class="reference external" href="https://www.jstor.org/stable/1969172">Steenrod 1947</a> , p.296).</p>
<p>As introduced previously, an information path <span class="math notranslate nohighlight">\(IP_k\)</span> of degree k on <span class="math notranslate nohighlight">\(I_k\)</span> landscape is defined as a sequence of elements of
the lattice that begins at the least element of the lattice (the identity-constant “0”), travels along edges from element to element of
increasing degree of the lattice and ends at the greatest element of the lattice of degree k (a piecewise linear function). The
first derivative of an <span class="math notranslate nohighlight">\(IP_k\)</span> path is minus the conditional mutual information. The critical dimension of an <span class="math notranslate nohighlight">\(IP_k\)</span> path
is the degree of its first minimum. A positive information path is an information path from 0 to a given <span class="math notranslate nohighlight">\(I_k\)</span> corresponding to a given
k-tuple of variables such that <span class="math notranslate nohighlight">\(I_k &lt; I_{k-1}  &lt; ... &lt; I_1\)</span> (a chain, total order).
A maximal positive information path is a positive information path of maximal length: it ends at minima of <span class="math notranslate nohighlight">\(I_k\)</span> along the path (a minima
of the free energy components quantified by <span class="math notranslate nohighlight">\(I_k\)</span>. In statistical terms, this minima is equivalent to a conditionnal independence: it means
that the conditional mutual information (slope) of the paths goes throug 0.
Those maximal paths identifies the maximal faces of the <span class="math notranslate nohighlight">\(I_k\)</span> complex and charaterize it, because a simplicial complex is uniquely determined
by the list of its maximal faces (intoduction to <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiRyeqK6PrrAhUDyxoKHSBBAcoQFjACegQIBRAB&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs00454-017-9865-z%3Fshared-article-renderer&amp;usg=AOvVaw2tpdusnNrLy9q4cqxB240g">simplicial homology</a> ). Hence, the set of all these paths defines uniquely the <span class="math notranslate nohighlight">\(I_k\)</span> complex (or minimum free energy complex).
An example of such a complex of dimension 4, with its information path <span class="math notranslate nohighlight">\(IP_k\)</span>, is given in this figure:</p>
<img alt="_images/info_complex.png" src="_images/info_complex.png" />
<p>Note that, as a result of classical Shannonian information inequalities, any complex of dimension 3 or below is necessarilly a simplex, indicating
that in information and statitics, 3 and 4 dimensional topology are also very special.</p>
</div>
<div class="section" id="poincare-shannon-machine">
<h3>Poincaré-Shannon Machine<a class="headerlink" href="#poincare-shannon-machine" title="Permalink to this headline">¶</a></h3>
<p>Information theory motivated the early stages of Machine Learning and Information sensory processing theories. The principle was self-resumed by Chaitin:
“Understanding is compressing”. Notably,  <a class="reference external" href="https://www.semanticscholar.org/paper/Some-informational-aspects-of-visual-perception.-Attneave/6d0198460198fdb49b89d1646049712b3a0683df">Attneave (1954)</a>
ennouciated the principles of efficient coding (with Barlow) in the following terms: the goal of sensory perception is to extract the redundancies and to find the
most compressed representation of the environment. Any kind of symmetry and invariance are information redundancies and Gestalt principles of perception
can be defined on information theoretic terms. This is basically illustrated by, Attneave’s famous cat and the topologically sounding Gestalt principle of
perceptual binding illustrated bellow:</p>
<img alt="_images/figure_Attneave-Gestalt_complete.png" src="_images/figure_Attneave-Gestalt_complete.png" />
<p>Since then Information theory has provided machine learning’s central functions: the loss functions: Maximum entropy is at the root of Jaynes and may statistical physic inference
model, maximum mutual information (infomax) was stated and studied  by Linsker, Nadal and Parga, and Bell and Sejnowsky and formalized ICA principles and Hebbian
plasticity, generalizing PCA to non-linear cases, Boltzmann Machine minimized the KL-divergence… untill current Deep Convolutional Neural Networks (CNN) that
basically minimize cross entropy or “deformed” functions of it like the focal loss (very close indeed to a “deformed probability”!). The principles stayed the same,
but Neural network architectures, data availability, computational power and software facilities increased enormously.</p>
<img alt="_images/loss_function.jpg" src="_images/loss_function.jpg" />
<p>For instance, Boltzmann Machines are reccurent neural networks of binary random variables with hidden layer, that can be formalized as a Markov random field.
Markov random fields are a small, positive, subcase of information structures (see <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/869">proposition 7 (Hu) PDF</a>).</p>
<p>The models developped here are called the Poincaré-Shannon machine in reverence to <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwinjPbMnvjrAhUKzYUKHStSA7gQgAMoAHoECAgQAg&amp;url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fwww.cs.utoronto.ca%2F~hinton%2Fabsps%2Fcogscibm.pdf%26hl%3Dfr%26sa%3DX%26ei%3Duo5nX8mHM72Ay9YP1OOAiAM%26scisig%3DAAGBfm0MtFqrPZRIBb9G16LNS5kfPdVoFw%26nossl%3D1%26oi%3Dscholarr&amp;usg=AOvVaw29iesHzi-bIRQnf2tYDIH1">Boltzmann Machine</a> ,
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjZ8J7GoPjrAhWhxYUKHZxbB74QFjABegQIBBAB&amp;url=http%3A%2F%2Fwww.gatsby.ucl.ac.uk%2F~dayan%2Fpapers%2Fhm95.pdf&amp;usg=AOvVaw1wOfAfLAIVYS83_2EO-6Fi">Helmholtz Machine</a>
and the original <a class="reference external" href="https://www.pnas.org/content/79/8/2554">Hopfield’s network</a> , since it implements simplicial homology (see <a class="reference external" href="http://analysis-situs.math.cnrs.fr/-Textes-originaux-.html">Poincaré’s Analysis Situs</a> , that arguably foundate algebraic topology)
and information theory in a single framework (see <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjtrqOXrsPrAhVCrxoKHcBDBrQQgAMoAHoECBIQAg&amp;url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fpure.mpg.de%2Frest%2Fitems%2Fitem_2383162_7%2Fcomponent%2Ffile_2456978%2Fcontent%26hl%3Dfr%26sa%3DX%26scisig%3DAAGBfm2dgGR4Ly92eRCfhrM1BgCnbIBvBA%26nossl%3D1%26oi%3Dscholarr&amp;usg=AOvVaw0ha99XPEPwgTiv3oMC7PTE">Shannon’s theory of communication</a> ,
that foundate information theory), applied effectively to empirical data.</p>
<p>The Poincaré-Shannon machine are generic feed forward Deep Neural Networks (DNN) model with a layered structure given by a chain complex (of random variables), e.g. imposed by algebraic topology,
and whose connections are given by the edges of the embedding lattice.
In the basic simplicial case developped computationnaly here, the rank of the layers of the DNN is the dimension of the faces of the complex, and the highest rank of the layers is the
dimension of the complex. As with usual DNN, the dimension of the analized patterns increases with the depth of the layers (see for illustration, <a class="reference external" href="https://arxiv.org/abs/1311.2901">figure 2 Zeiler and Fergus 2013</a>)
The neurons are random variables, and are whatever measurable functions (linear, non linear), hence covering a “fairly” large class of functions (notably, using the  <a class="reference external" href="https://www.researchgate.net/publication/239065757_A_Model_of_Set_Theory_in_Which_Every_Set_of_Reals_is_Lebesgue_Measurable">Solovay’s axiomatic of set theory</a>, all functions
are measurable). In the general (and computationally hard) setting of general information strutures, that considers the lattice of partitions (cf. section “how infotopo works”), the Poincaré-Shannon machine are
Universal Classifiers, in the sens that a partition corresponds exactly to an equivalence class and in theory such a model would span all classifications up to equivalence).
This topological structure allows, and de facto implements the fact, that neural layers are not necessarilly serial as in current DNN, but can be parralel.
Such  architectures are well known in real sensory cortical systems, for example the ventral and dorsal visual streams in human cortex would corresponds
to two facets of the human brain complex with two (at least partially disjoint information paths) and analyze conditionally independent features of the input such as the “where and what”
(dorso and ventral, respectively <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/881">PDF</a>).
Hence one of the interest of such deep model is that its architecture (number of layers, number of neurons at each layer, connectivity) and computation is fully mastered and understood, as far as
simplicial complexes can be: it can be understood as an algebrization of neural networks (there are other very interesting approachs of such topic (not probabilistic as here),
see for example the <a class="reference external" href="https://arxiv.org/pdf/1511.00255.pdf">neural rings (Curto and Youngs 2013-2020)</a> or <a class="reference external" href="https://arxiv.org/abs/1605.04463">Morisson et al. 2016</a> <a class="reference external" href="https://arxiv.org/pdf/1804.01487.pdf">Morisson and Curto 2018</a> ).</p>
<p>Beside this architectural difference with usual DNN, the second important difference is that the learning rule is a “forward propagation”, imposed by the cohomological
“direction”, whereas usual DNN implements a backpropagation learning rule (homological “direction”) which implements basically the chain rule of derivation (<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiVgubR3PjrAhWGDxQKHU7XAOMQFjABegQIBRAB&amp;url=https%3A%2F%2Fwww.gwern.net%2Fdocs%2Fstatistics%2Fdecision%2F1960-kelley.pdf&amp;usg=AOvVaw3kqby-zRKHaI0gxZPh8Dax">Kelley 1960</a> ,
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjo4POW4PjrAhWrAGMBHbFRBY8QFjAAegQIBBAB&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-85.pdf&amp;usg=AOvVaw0IUPPzZ_XUtTjFjzpgm7gG">Le Cun 1985</a>,
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjI8JmG3fjrAhXq6eAKHfP6CeoQFjACegQIAxAB&amp;url=https%3A%2F%2Fcore.ac.uk%2Fdownload%2Fpdf%2F82751002.pdf&amp;usg=AOvVaw2uSnnLkJUmd9ofdIxpjN9E">Dreyfus 1962</a>,
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjswqz33fjrAhVGKBoKHXrBC9sQFjACegQIAxAB&amp;url=https%3A%2F%2Fwww.iro.umontreal.ca%2F~vincentp%2Fift3395%2Flectures%2Fbackprop_old.pdf&amp;usg=AOvVaw0EfTJmB5LLenmX5JrYjp-O">Rumelhart et al. 1986</a>).
The information topology take profit of the coboundary nature of <span class="math notranslate nohighlight">\(I_k\)</span> functions, a (discrete in the present particular case) statistical analog of differential operator.
This means that there is no descent as in the usual DNN implementation, but that computation of those <span class="math notranslate nohighlight">\(I_k\)</span> and conditional <span class="math notranslate nohighlight">\(I_k\)</span> implements the descent.
Notably, the introduction of the multiplicity decomposition of “energy functions” formalizes learning in neural networks in terms of a combinatorial
family of analytically independent functions <span class="math notranslate nohighlight">\(I_k\)</span> (moreover with independent gradients) on the probability simplex (<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiG2ODn5fjrAhVDx4UKHfSNATUQgAMoAHoECAgQAg&amp;url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0019995875800040%2Fpdf%253Fmd5%253D86adf67be6e855ec022029450d43b0ce%2526pid%253D1-s2.0-S0019995875800040-main.pdf%26hl%3Dfr%26sa%3DX%26ei%3DZdlnX8qEO7OTy9YP9bWDyAc%26scisig%3DAAGBfm0fipxDddOGu6177-TJWIh6DFJuWg%26nossl%3D1%26oi%3Dscholarr&amp;usg=AOvVaw3bBFjSpCiBOnsAeG3lIzOX">Han 1975</a>
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiUwdj75fjrAhUG9IUKHfuqAkAQFjACegQIBxAB&amp;url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0019995878902759%2Fpdf%3Fmd5%3D18d2eec90c7b3dd3009f70a8bb39eb80%26pid%3D1-s2.0-S0019995878902759-main.pdf%26_valck%3D1&amp;usg=AOvVaw1HIUfffUm-y61YrosK2XBv">Han 1978</a> Theorem 4 in <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/869">PDF</a>): instead of a single energy and
associated gradient descent, mutual information provides a multiplicity of gradients.
The following illustration presents the DNN architecture associated with the previous example of an <span class="math notranslate nohighlight">\(I_4\)</span> complex:</p>
<img alt="_images/figure_deep_neural_net.png" src="_images/figure_deep_neural_net.png" />
</div>
</div>
<div class="section" id="unsupervised-topological-learning">
<h2>Unsupervised topological learning<a class="headerlink" href="#unsupervised-topological-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="causality-challenge-dataset">
<h3>Causality challenge dataset<a class="headerlink" href="#causality-challenge-dataset" title="Permalink to this headline">¶</a></h3>
<p>We will illustrate the computation of free energy complex (or <span class="math notranslate nohighlight">\(I_k\)</span> complex) on the synthetic dataset <a class="reference external" href="http://www.causality.inf.ethz.ch/data/LUCAS.html">LUCAS  (LUng CAncer Simple set)</a>
of the  <a class="reference external" href="http://www.causality.inf.ethz.ch/challenge.php">causality challenge</a>. Before trying the code on your computer, you will have to download the file “lucas0_train.csv”
and to save it on your hard disk (here at the path “/home/pierre/Documents/Data/lucas0_train.csv”), and to put your own path in the following commands with the initialisation
of infotopo’s parameters.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;/home/pierre/Documents/Data/lucas0_train.csv&quot;</span><span class="p">)</span>  <span class="c1"># csv to download at http://www.causality.inf.ethz.ch/data/LUCAS.html</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">information_topo</span> <span class="o">=</span> <span class="n">infotopo</span><span class="p">(</span><span class="n">dimension_max</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">dimension_tot</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">sample_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">work_on_transpose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="n">nb_of_values</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                        <span class="n">sampling_mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="n">deformed_probability_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="n">supervised_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="n">forward_computation_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="n">dim_to_rank</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">number_of_max_val</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>The dataset is composed of 11 variables: 1: Smoking, 2: Yellow_Fingers, 3: Anxiety, 4: Peer_Pressure, 5: Genetics, 6: Attention_Disorder, 7: Born_an_Even_Day,
8: Car_Accident, 9: Fatigue, 10: Allergy, 11: Coughing and the 12th variable of iterest: Lung cancer.
The (buildin) causality chain relations among those varaibles follow this schema:</p>
<img alt="_images/causality_schema_LUCAS0.png" src="_images/causality_schema_LUCAS0.png" />
</div>
<div class="section" id="id2">
<h3>Information Complexes<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>To compute (approximation) of the information complex (free-energy complex), you can use the following command:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ninfomut</span><span class="p">,</span> <span class="n">Nentropie</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">information_topo</span><span class="o">.</span><span class="n">information_complex</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
</pre></div>
</div>
<p>The method “fit” is just a wrapper of the methods “simplicial_entropies_decomposition” and “simplicial_infomut_decomposition”, that is introduced to correspond to
the usual methods of scikit-learn, keras, tensorflow (…). The set of all paths of degree-dimension k is intractable computationally (complexity in <span class="math notranslate nohighlight">\(\mathcal{O}(k!)\)</span> ).
In order to bypass this issue, the current method “information_complex” computes a fast local algorithm that selects at each element of degree k of a path, the
positive information path with maximal or minimal <span class="math notranslate nohighlight">\(I_{k+1}\)</span> value (equivalently, extremal conditional mutual informations) or stops whenever
<span class="math notranslate nohighlight">\(X_k.I_{k+1} \leq 0\)</span> and ranks those paths by their length. The justification of this elementary heuristic is that it should capture the paths with the most interesting
tuples, e.g the one highest anf lowest <span class="math notranslate nohighlight">\(I_{k}\)</span>. No doubt that this approximation is rought and shall be improved in future (to be done).
The result on the causality challenge dataset is:</p>
<img alt="_images/causality_info_paths.png" src="_images/causality_info_paths.png" />
<p>and it prints the following paths:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">maximal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">1</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">minimal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">1</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">maximal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">2</span>  <span class="ow">is</span> <span class="p">:[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">minimal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">2</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">maximal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">3</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">minimal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">3</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">maximal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">4</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">minimal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">4</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">maximal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">5</span>  <span class="ow">is</span> <span class="p">:[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">The</span> <span class="n">path</span> <span class="n">of</span> <span class="n">minimal</span> <span class="n">mutual</span><span class="o">-</span><span class="n">info</span> <span class="n">Nb</span> <span class="mi">5</span>  <span class="ow">is</span> <span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span> <span class="n">etc</span><span class="o">..</span>
</pre></div>
</div>
<p>The first maximal path [5, 12, 11, 9, 8, 6, 2, 1, 10, 4]  as length 10 and the first 5 variables corresponds to the longest causal chain of the data as illustrated bellow.
The fact that the resulting path is so long is likely due to the generating algorithm used for Lucas, and the last [6,2,1,10,4] errors could be removed by statistical test
thresholding on conditional mutual information values. The next maximal paths fail to identify the other long causal chain of the data, probably as a consequence of
the rought approximation used by the algorithm. The First two minimal paths [7, 2, 11] and [3, 4, 1] identifies unrelated variables or multiple cause causality scheme.</p>
<img alt="_images/causality_info_paths_results.png" src="_images/causality_info_paths_results.png" />
</div>
<div class="section" id="digits-dataset">
<h3>Digits Dataset<a class="headerlink" href="#digits-dataset" title="Permalink to this headline">¶</a></h3>
<p>In order to illustrate unsupervised and supervised learning methods we will now turn to the classical dataset of Digits NIST, which is a common toy dataset to train and test
machine learning models. We load it as previously using the symapthic Scikit-learn repository:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax_array</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">ax_array</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray_r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">frame_on</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">h_pad</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">w_pad</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<p>It prints the following complete description of the dataset:</p>
<pre class="literal-block">Optical recognition of handwritten digits dataset
--------------------------------------------------

<strong>Data Set Characteristics:</strong>

:Number of Instances: 5620
:Number of Attributes: 64
:Attribute Information: 8x8 image of integer pixels in the range 0..16.
:Missing Attribute Values: None
:Creator: E. Alpaydin (alpaydin '&#64;' boun.edu.tr)
:Date: July; 1998

This is a copy of the test set of the UCI ML hand-written digits datasets  <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits</a>

The data set contains images of hand-written digits: 10 classes where each class refers to a digit.

Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a
total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of
4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range
0..16. This reduces dimensionality and gives invariance to small distortions.

For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G., T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.

.. topic:: References

  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
    Graduate Studies in Science and Engineering, Bogazici University.
  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.
  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of
    Electrical and Electronic Engineering Nanyang Technological University. 2005.
  - Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000.</pre>
<p>And illustrates the dataset with the following sample of digits pictures:</p>
<img alt="_images/digits_dataset_sample.png" src="_images/digits_dataset_sample.png" />
</div>
<div class="section" id="adaptive-computational-complexity">
<h3>Adaptive computational complexity<a class="headerlink" href="#adaptive-computational-complexity" title="Permalink to this headline">¶</a></h3>
<p>The images of digits dataset are 8*8 pixels, meaning that we have 64 Random Variables or dimensions: this will introduce us to the problemantic of high dimensional space
(here not so high) and of computational complexity. In this case the information simplicial structure has <span class="math notranslate nohighlight">\(2^{64}\)</span> information estimations to compute, which is much
too big, and we propose a partial exploration that will stop the computation at a given dimension  “dimension_max”.
This methods of partial exploration allows to adapt the computational (time) complexity of the algorithm to a reasonable complexity given your computational ressources and the dimension of the dataset.
As we have seen, when increasing the dimension of the dataset, the raw computation potentially grows as <span class="math notranslate nohighlight">\(\mathcal{O}(2^n)\)</span>. In order to master and circumvince this
problem, a partial exploration of information structures as been written, allowing to explore only all the k first dimensions with <span class="math notranslate nohighlight">\(n \geq k\)</span>. This is acheived by
setting the parametter “dimension_max” to k and “forward_computation_mode” to “True”. For example, setting “dimension_max=2” will restrict the computation to the <span class="math notranslate nohighlight">\(\binom{n}{1} = n\)</span> and
the <span class="math notranslate nohighlight">\(\binom{n}{2} = n!/(2!(n-2)!) = n.(n-1)/2\)</span> estimations of information, which is the (symetric) usual complexity  <span class="math notranslate nohighlight">\(\mathcal{O}(n^2)\)</span> of metric or graph
based machine learning algorithm.
Setting to 3, there will be <span class="math notranslate nohighlight">\(\binom{n}{1} + \binom{n}{2} + \binom{n}{3}\)</span> estimations of information giving a complexity in <span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span> etc…  Of course, we gain what we loose, and the deployement of infotopo on GPU should give a bit more
of ressources  (currently failed).
In 64 dimensions, choosing an exploration of the 5 first dimensions (64+2016+41664+635376+7624512=8303632 estimations) gives a reasonably long computation of several hours on a personal laptop (has acheived here)
To set such exploration, we initialize infotopo using the commands:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span> <span class="o">=</span> <span class="n">infotopo</span><span class="p">(</span><span class="n">dimension_max</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                            <span class="n">dimension_tot</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">sample_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">work_on_transpose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                            <span class="n">nb_of_values</span> <span class="o">=</span> <span class="mi">17</span><span class="p">,</span>
                            <span class="n">sampling_mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">deformed_probability_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                            <span class="n">supervised_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                            <span class="n">forward_computation_mode</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                            <span class="n">dim_to_rank</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">number_of_max_val</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>For data scientist used to deep learning terminology, this intialization corresponds to building the model, although extremely simple.
As you see, the whole structure of the model is fully constrained by the dataset’s embedding dimension, the dimension max (computational complexity restriction), and the number of values chosen for the variables (with other purely
computational internal parameter).</p>
</div>
</div>
<div class="section" id="id3">
<h2>Unsupervised topological learning<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[H_1=H(X_{j};P)=k\sum_{x \in [N_j] }p(x)\ln p(x)\]</div>
</div>
</div>


    </div>
      
  </div>
</div>
    <hr>
    <footer>
        <br>
        <div class="container">
            <ul class="list-inline">
                <li style="font-weight:bold"><a href="genindex.html">Index</a></li>
                <li>·</li>
                <li style="font-weight:bold"><a href="https://github.com/pierrebaudot/infotopopy">Github</a></li>
                <li>·</li>
                <li style="font-weight:bold"><a href="references.html">References</a></li>
            </ul>
         </div>
        <!-- <br> -->
        <p>&copy; Copyright 2020, Pierre Baudot.</p>
    </footer>

  </body>
</html>