<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>How to Use InfoTopo &#8212; infotopo 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" />
    <link rel="prev" title="Topological Learning" href="method.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          infotopo</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="method.html">Method</a></li>
                <li><a href="#">Tutorial</a></li>
                <li><a href="install.html">Install</a></li>
                <li><a href="api.html">API</a></li>
                <li><a href="auto_examples/index.html">Examples</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption"><span class="caption-text">User Guide / Tutorial:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="method.html">Topological Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="method.html#topological-learning-principles">Topological Learning principles</a><ul>
<li class="toctree-l3"><a class="reference internal" href="method.html#information-complexes">Information Complexes</a></li>
<li class="toctree-l3"><a class="reference internal" href="method.html#poincare-shannon-machine">Poincaré-Shannon Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="method.html#unsupervised-topological-learning">Unsupervised topological learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="method.html#causality-challenge-dataset">Causality challenge dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="method.html#id2">Information Complexes</a></li>
<li class="toctree-l3"><a class="reference internal" href="method.html#digits-dataset">Digits Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="method.html#adaptive-computational-complexity">Adaptive computational complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="method.html#id3">Unsupervised topological learning</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to Use InfoTopo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#iris-data">Iris data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#iris-dataset">Iris dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#entropy">Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mutual-information">Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#information-networks">Information Networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#diabetes-data">Diabetes data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#diabetes-dataset">Diabetes dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#total-correlation">Total correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">Information Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mean-information-path">Mean Information path</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conditional-transfer-informations">Conditional (transfer) Informations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#entropy-vs-energy">Entropy Vs. Energy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#information-distance">Information distance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#standard-installation">Standard installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-the-most-up-to-date-version">Install the most up-to-date version</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/tutorial.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="how-to-use-infotopo">
<span id="tutorial"></span><h1>How to Use InfoTopo<a class="headerlink" href="#how-to-use-infotopo" title="Permalink to this headline">¶</a></h1>
<p>Infotopo is a general Machine Learning set of tools gathering Topology
(Cohomology and Homotopy), statistics and information theory
(information quantifies statistical structures generically) and
statistical physics.
It provides a matheamticaly formalised expression of deep network and learning,
and propose anuspervised or supervised learning mode (as a special case of the first).
It allows a simple and autonatic exploration of the data structures, dimension reduction
and supervised or unsupervised classification.</p>
<p>The raw methods are computationnally consuming due to the intrinsic combinatorial
nature of the topological tools, even in the simplest case of a simplicial case
(the general case is based on the much broader partition combinatorics) the
computational complexity is of the order of <span class="math notranslate nohighlight">\(2^n\)</span> .
As a consequence, an important part of the tools and methods are dedicated
to overcome this extensive computation. Among the possible strategies and
heuristics used or currently developped, are:</p>
<ul class="simple">
<li><p>restrict to simplicial cohomology and combinatorics (done here).</p></li>
<li><p>possible exploration of only the low dimensional structures (done here).</p></li>
<li><p>possible exploration of only most or least informative paths (done here).</p></li>
<li><p>possible restriction 2nd degree-dimension statistical interactions:</p></li>
</ul>
<p>what is computed here is the equivalent of the Cech complex (with all degree-
dimension computed), and such restriction is equivalent to computing the Vietoris-Rips
complex (in development).
* compute on GPU (in development).</p>
<p>As a result, for this 0.1 version of the software, and for computation with
commercial average PC, we recommand to analyse up to 20 variables (or dimensions)
at a time in the raw brut-force approach (see performance section).</p>
<p>We now present some basic example of use, inspiring our presentation from
the remarkable presentation of <a class="reference external" href="https://umap-learn.readthedocs.io/en/latest/">UMAP by McInnes.</a>
We first import some few tools: some of the datasets available in sklearn, seaborn to
visualise the results, and pandas to handle the data.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span><span class="p">,</span> <span class="n">load_digits</span><span class="p">,</span> <span class="n">load_boston</span><span class="p">,</span> <span class="n">load_diabetes</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">timeit</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="s1">&#39;notebook&#39;</span><span class="p">,</span> <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">:(</span><span class="mi">14</span><span class="p">,</span><span class="mi">10</span><span class="p">)})</span>
</pre></div>
</div>
<div class="section" id="iris-data">
<h2>Iris data<a class="headerlink" href="#iris-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="iris-dataset">
<h3>Iris dataset<a class="headerlink" href="#iris-dataset" title="Permalink to this headline">¶</a></h3>
<p>The first example of dataset application we will present is the <a class="reference external" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris
dataset</a>. It is
a very small dataset composed of 4 Random-Variables or dimensions that
quantify various petals and sepals observables of 3 different species of
Iris flowers, like petal length, for 150 flowers or points (50 for each
species). In the context of Infotopo it means that dimension_tot = 4
and sample_size = 150 (we consider all the points), and as the dimension
of the data set is small we will make the complete analysis of the
simplicial structure of dependencies by setting the maximum dimension
of analysis to dimension_max = dimension_tot. We also set the other
parameters of infotopo to approriate, as further explained.
We can load the iris dataset from sklearn.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>

<span class="n">dimension_max</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dimension_tot</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sample_size</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">nb_of_values</span> <span class="o">=</span><span class="mi">9</span>
<span class="n">forward_computation_mode</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">work_on_transpose</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">supervised_mode</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">sampling_mode</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">deformed_probability_mode</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Iris Plants Database
====================

Notes
-----
Data Set Characteristics:
    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

This is a copy of UCI ML iris datasets.
http://archive.ics.uci.edu/ml/datasets/Iris

The famous Iris database, first used by Sir R.A Fisher

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher&#39;s paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

References
----------
   - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot;
     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to
     Mathematical Statistics&quot; (John Wiley, NY, 1950).
   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
</pre></div>
</div>
<p>As visualizing data in 4 dimensions or more is hard or not possible, we can first
plot all the pairwise scatterplot matrix to present the pairwise correlations and
dependencies between the variables, using Seaborn and pandas dataframe.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">iris_df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">iris_df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;species&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="_images/iris_pairwise_scatter.png" src="_images/iris_pairwise_scatter.png" />
<p>All those 2D views gives a rought but misleading idea of what the data looks
like in high dimensions since, as we will see, some fully emergent
statistical dependences (called synergy in the original work of Bialek’s team) can appear in higher dimension which are
totally unobservable in those 2D views. However such 2D views gives a fair
visual estimation of how much each pairs of variale covary, the correlation
coefficient and its generalization to non-linear relations, the pairwise
Mutual Information (I2). In Topological Data Analysis (TDA) terms, it gives rought
idea of what the skeleton of a Vietoris-Rips (information or correlation) complex
of the data could be.
We will see how to go beyond this pairwise statistical interaction case, and how
we can unravel some purely emergent higher dimensional interations. Along this
way, we will see how to compute and estimate all classical information functions,
multivariate Entropies, Mutual Informations and Conditional Entropies and
Mutual Informations.</p>
</div>
<div class="section" id="entropy">
<h3>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h3>
<p>To use infotopo we need to first construct a infotopo object from
the infotopo package. This makes a lot of same word, information is a
functor, a kind of general application or map, that could be either a
function or a class. So let’s first import the infotopo library, we a set
of specifications of the parametters (cf. section parameters, some of them
like dimension_max = dimension_tot and sample_size have been fixed
previously to the size of the data input matrix).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">infotopo</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span> <span class="o">=</span> <span class="n">infotopo</span><span class="o">.</span><span class="n">infotopo</span><span class="p">(</span><span class="n">dimension_max</span> <span class="o">=</span> <span class="n">dimension_max</span><span class="p">,</span>
                            <span class="n">dimension_tot</span> <span class="o">=</span> <span class="n">dimension_tot</span><span class="p">,</span>
                            <span class="n">sample_size</span> <span class="o">=</span> <span class="n">sample_size</span><span class="p">,</span>
                            <span class="n">work_on_transpose</span> <span class="o">=</span> <span class="n">work_on_transpose</span><span class="p">,</span>
                            <span class="n">nb_of_values</span> <span class="o">=</span> <span class="n">nb_of_values</span><span class="p">,</span>
                            <span class="n">sampling_mode</span> <span class="o">=</span> <span class="n">sampling_mode</span><span class="p">,</span>
                            <span class="n">deformed_probability_mode</span> <span class="o">=</span> <span class="n">deformed_probability_mode</span><span class="p">,</span>
                            <span class="n">supervised_mode</span> <span class="o">=</span> <span class="n">supervised_mode</span><span class="p">,</span>
                            <span class="n">forward_computation_mode</span> <span class="o">=</span> <span class="n">forward_computation_mode</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we will compute all the simplicial semi-lattice of marginal and joint-entropy,
that contains <span class="math notranslate nohighlight">\(2^n\)</span> elements including the unit 0 reference measure element.
The marginal <span class="math notranslate nohighlight">\(H_1\)</span> entopies are defined as classicaly by <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjtrqOXrsPrAhVCrxoKHcBDBrQQgAMoAHoECBIQAg&amp;url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fpure.mpg.de%2Frest%2Fitems%2Fitem_2383162_7%2Fcomponent%2Ffile_2456978%2Fcontent%26hl%3Dfr%26sa%3DX%26scisig%3DAAGBfm2dgGR4Ly92eRCfhrM1BgCnbIBvBA%26nossl%3D1%26oi%3Dscholarr&amp;usg=AOvVaw0ha99XPEPwgTiv3oMC7PTE">Shannon</a> :</p>
<div class="math notranslate nohighlight">
\[H_1=H(X_{j};P)=k\sum_{x \in [N_j] }p(x)\ln p(x)\]</div>
<p>and the multivariate joint-entropies <span class="math notranslate nohighlight">\(H_k\)</span> just generalise the preceding to k variables:</p>
<div class="math notranslate nohighlight">
\[H_k= H(X_{1},...,X_{k};P)=  k\sum_{x_1,...,x_k\in [N_1\times...\times N_k]}^{N_1\times...\times N_k}p(x_1.....x_k)\ln p(x_1.....x_k)\]</div>
<p>The figure below give the usual Venn diagrams representation of set theoretic unions
and the corresponding semi-lattice of joint Random Variables and Joint Entropies, together
with its correponding simplicial representation, for 3 (top) and 4 variables-dimension
(bottom, the case of the iris dataset with 2 power 4 joint random variables). This correspondence
of joint-information with the semi-lattice of union was formalized by <a class="reference external" href="https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing">Hu Kuo Ting</a> .
The edges of the lattice are in one to one correspondence with conditional entropies.</p>
<img alt="_images/figure_lattice.png" src="_images/figure_lattice.png" />
<p>To do this we will call simplicial_entropies_decomposition, that gives in output
all the joint entropies in the form of a dictionary with keys given by the tuple of
the joint variables (ex: (1,3,4)) and  with values the joint or marginal entropy in bit
(presented below).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Nentropie</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">simplicial_entropies_decomposition</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{(</span><span class="mi">4</span><span class="p">,):</span> <span class="mf">2.9528016441309237</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,):</span> <span class="mf">2.4902608474907497</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,):</span> <span class="mf">2.5591245822618114</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,):</span> <span class="mf">2.8298425472847066</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">3.983309507504916</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">4.798319817958397</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">4.83234271597051</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span> <span class="mf">4.437604597473526</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span> <span class="mf">4.2246575340121835</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mf">4.921846615158947</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">5.561696151051504</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">5.426426190681815</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">6.063697650692486</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span> <span class="mf">5.672729631265195</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span> <span class="mf">6.372515544003377</span><span class="p">}</span>
</pre></div>
</div>
<p>Such dictionary is hard to read; to allow a relevant visualization of the
the simplicial entropy structure, the function simplicial_entropies_decomposition
also plots the Entropy landscapes. Entropy landscapes provides a representation of the lattice
of joint (<span class="math notranslate nohighlight">\(H_k\)</span>) and conditional entropies (noted as the action of Y <span class="math notranslate nohighlight">\(Y.H_k\)</span>,
for <span class="math notranslate nohighlight">\(H(X_1,...,X_k|Y)\)</span>) that ranks the joint variables as a function of their entropy value
and of the rank-dimensions as illustrated in the figure below:</p>
<img alt="_images/Principles_TIDA.png" src="_images/Principles_TIDA.png" />
<p>An Entropy of Information Path is a sequence of inclusive tuples of increasing dimensions and follows
the edges of the semi-lattice, and the slope of such a path is exactly minus the conditional-entropy,
as a basic representation of the fundamental chain rule of Entropy.</p>
<p>While the total dimension n (dimension_tot) of the analysis increases, the number of subsets of k
variables (or k-tuples) increases combinatorially, following the binomial coefficient C(n,k).
It hence becomes rapidly fully impractical to vizualize, plot and to differentiate the C(n,k) values of
entropy obtained in dimension k. The Entropy landscapes hence plot the histograms of entropy values as a
function of the dimension-rank k, and the number of bins of the histograms is imposed by the parameter
nb_bins_histo. The count of the number of subsets having entropy values in the bin range of the histograms
is represented by a color code in the entropy landscapes. Hence, Entropy Landscapes shall be understood as
(unormalised..but it could be normalised) entropy measure densities histograms (there is interesting further
theoretical and applied development upon this point, since entropy functions obey axioms of measure: one
could legitamely investigate entropies of entropies, a kind of complexity of information landscapes, see
<a class="reference external" href="https://www.mdpi.com/1099-4300/19/10/550">Hsu et al.</a> ).</p>
<p>To plot the Entropy Landscapes and the distribution of entropy values for each dimension-rank k, we use
the “entropy_simplicial_lanscape” command as following:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span><span class="o">.</span><span class="n">entropy_simplicial_lanscape</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">)</span>
</pre></div>
</div>
<p>On the example of Iris dataset, the Entropy Landscape we obtain look like this:</p>
<img alt="_images/figure_entropy_simplicial_lattice.png" src="_images/figure_entropy_simplicial_lattice.png" />
<p>In this low dimensional case (dimension_tot = 4), the landscapes are very low informative (poor information
structure) and the histrograms have low meaning, since there is only one subset-k-tuple per bin value, and hence only
one color (here the green value of 1). The Entropy Landscape themselfs are quite poor in information, joint-entropy is
monotonically increasing along entropy path, a direct consequence of conditional-entropy positivity (concavity argument)
which is moreover the basic fact at the origin of the basic topological expression of the 2nd law of thermodynamic [3].
As a consequence, we usually do not uncover a lot of usefull information on the datas structure from those Entropy Landscape,
at the exception of curse of dimensionality quantification and in some cases, (assymptotic) entropy rates (to do).
Basically, joint-entropy quantifies “randomness” (in a non formal definition of the word), uncertainty, or how much the
data points spreads in the dimensions of the variables. Hence low entropies shall be intrepreted as “localised” densities
of data points or sparsness of the probability density histograms (also not in the usual kurtosis sens).</p>
<p>In any entropy or information function estimation, it is necessary to check that the number of sample is sufficient to
provide a faithfull estimate, to avoid the sampling problem also called “curse of dimension”. The command
“entropy_simplicial_lanscape” also computes the maximal dimension above which the estimation becomes too inacurate and
shall not be interpreted. This is explained in more details in the section “curse_of_dimension_and_statistical_dependencies_test”.</p>
</div>
<div class="section" id="mutual-information">
<h3>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this headline">¶</a></h3>
<p>Now, let’s have a look at the statistical dependencies structures in the dataset by computing the Mutual-Information lanscapes
which principle is depicted in the preceding figure and that basically plots k-dimensional multivariate Mutual Informations (<span class="math notranslate nohighlight">\(I_k\)</span>) in the same
way as Entropy Landscapes. Pairwise Mutual Information <span class="math notranslate nohighlight">\(I_2\)</span> is defined as usual following <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjtrqOXrsPrAhVCrxoKHcBDBrQQgAMoAHoECBIQAg&amp;url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fpure.mpg.de%2Frest%2Fitems%2Fitem_2383162_7%2Fcomponent%2Ffile_2456978%2Fcontent%26hl%3Dfr%26sa%3DX%26scisig%3DAAGBfm2dgGR4Ly92eRCfhrM1BgCnbIBvBA%26nossl%3D1%26oi%3Dscholarr&amp;usg=AOvVaw0ha99XPEPwgTiv3oMC7PTE">Shannon</a> :</p>
<div class="math notranslate nohighlight">
\[I_2=I(X_{1};X_{2};P)=k\sum_{x_1,x_2\in[N_1\times N_2]}^{N_1\times N_2}p(x_1.x_2)\ln \frac{p(x_1)p(x_2)}{p(x_1.x_2)}\]</div>
<p>They generalize to the multivariate mutual informations, <span class="math notranslate nohighlight">\(I_k\)</span>, as alternated functions of entropies, following <a class="reference external" href="https://drive.google.com/file/d/1Cpem9LVFYNScAcihBnqw7IRUjnW-wU04/view?usp=sharing">McGill</a> and <a class="reference external" href="https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing">Hu Kuo Ting</a>.</p>
<div class="math notranslate nohighlight">
\[I_k(X_1,...,X_k;P)=\sum_{i=1}^{k}(-1)^{i-1}\sum_{I\subset [k];card(I)=i}H_i(X_I;P)\]</div>
<p>For example:</p>
<div class="math notranslate nohighlight">
\[I_3=H(X_1)+H(X_2)+H(X_3)-H(X_1,X_2)-H(X1,X_3)-H(X_2,X_3)+H(X_1,X_2,X_3)\]</div>
<p><a class="reference external" href="https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing">Hu Kuo Ting</a> showed the correspondence of <span class="math notranslate nohighlight">\(I_k\)</span> with set intersections
semi-lattice (of finite measurable functions), and we hence have just like with entropy the following information structure, corresponding to intections on Venn
diagrams:</p>
<img alt="_images/informationfunctions.png" src="_images/informationfunctions.png" />
<p>The other functions that quantifies multivariate depence are Total Correlations, <span class="math notranslate nohighlight">\(G_k\)</span> (<a class="reference external" href="http://www.neuralmachines.com/references/correlation.pdf">Watanabe</a> , see section diabetes data)
, or total free energy, or Integrated Information (<a class="reference external" href="http://www.neuralmachines.com/references/correlation.pdf">Tononi and Edelman</a> ) which are the Kullback-Leibler Divergence between the full joint-entropy and its marginals product,
for example, <span class="math notranslate nohighlight">\(G_3=H(X_1)+H(X_2)+H(X_3)-H(X_1,X_2,X_3)\)</span>:</p>
<div class="math notranslate nohighlight">
\[G_k= G_k(X_1;...X_k;P)=\sum_{i=1}^k H(X_i) - H(X_1;...X_k)\]</div>
<p>Whereas, <span class="math notranslate nohighlight">\(G_k\)</span> quantifies the total interactions, <span class="math notranslate nohighlight">\(I_k\)</span> quantify the contribution of the kth interaction. Notably, we have the theorems
that state that n variables are independent if and only if  <span class="math notranslate nohighlight">\(G_n =0\)</span>, and n variables are independent if and only if  all the <span class="math notranslate nohighlight">\(2^n-n-1\)</span>
<span class="math notranslate nohighlight">\(I_k\)</span> functions with <span class="math notranslate nohighlight">\(k \geq 2\)</span> are null (e.g. <span class="math notranslate nohighlight">\(I_k\)</span> provides a refined independence measure <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/869">PDF</a>).
In contrast with <span class="math notranslate nohighlight">\(G_k\)</span>, <span class="math notranslate nohighlight">\(I_k\)</span> can be negative for <span class="math notranslate nohighlight">\(k \geq 3\)</span>, a phenomenon called synergy in the original study of Brenner et al.
Considering the old goal of expressing all of physics in terms of information, following Brillouin, Jaynes, Wheeller (…), for <cite>k geq 2</cite>,
<span class="math notranslate nohighlight">\(G_k\)</span> corresponds to a Free-Energy functional of a k interacting body system, while the  <span class="math notranslate nohighlight">\(I_k\)</span> quantifies the contribution of the
k-bodies interaction to this total free energy. The <span class="math notranslate nohighlight">\(I_1\)</span> component is the internal energy:</p>
<div class="math notranslate nohighlight">
\[H_k(X_1,..,X_k;P_N)=E(X_1,..,X_k;P_N)-G(X_1,..,X_k;P_N)=E-G\]</div>
<p>The Free-energy decomposes itself as an alternated sum of <span class="math notranslate nohighlight">\(I_k\)</span> :</p>
<div class="math notranslate nohighlight">
\[G_k =\sum_{i=2}^{k}(-1)^{i}\sum_{I\subset [n];card(I)=i}I_i(X_I;P)\]</div>
<p>To plot the Information Landscapes and the distribution of <span class="math notranslate nohighlight">\(I_k\)</span> values for each dimension-rank k, we use
the “entropy_simplicial_lanscape” command as following:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span><span class="o">.</span><span class="n">mutual_info_simplicial_lanscape</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
</pre></div>
</div>
<p>On the example of Iris dataset, the Entropy Landscape we obtain look like this:</p>
<img alt="_images/iris_info_landscapes.png" src="_images/iris_info_landscapes.png" />
<p>To obtain the first m k-tuples with maximum and minimum value in dimension k, and if the dimension is 2,3 or 4 plot the data points in the
corresponding k-subspace (the 4th dimension is represented by a color code), we use the “display_higher_lower_information”.
For exmaple, plotting the 2 first maximum and minimum in dimension (framed in red and blue respectively in the last figure), we use
the following command:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span> <span class="o">=</span> <span class="n">infotopo</span><span class="p">(</span><span class="n">dim_to_rank</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">number_of_max_val</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">dico_max</span><span class="p">,</span> <span class="n">dico_min</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_information</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>On the example of Iris dataset, we obtain the two pairs of variables (3,4) and (1,3) that are the most statistically dependent (“correlated”):</p>
<img alt="_images/iris_max_I2.png" src="_images/iris_max_I2.png" />
<p>And we obtain the two pairs of variables (1,2) and (2,3) that are the less statistically dependent (“uncorrelated”):</p>
<img alt="_images/iris_min_I2.png" src="_images/iris_min_I2.png" />
<p>Whenever the dimension to study is more than 4, the function only retreives the dictionaries of the first maximum and minimum tuples (to print).</p>
</div>
<div class="section" id="information-networks">
<h3>Information Networks<a class="headerlink" href="#information-networks" title="Permalink to this headline">¶</a></h3>
<p>In biology (e.g “omic”), neuroscience (e.g “neural network”) and social science (e.g “social network”), it is common and helpfull to conceive and
visualize the one and two dimensional results as (first degree) networks. To visualize the Information Networks, we use the
“mutual_info_pairwise_network” as following:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">adjacency_matrix_mut_info</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">mutual_info_pairwise_network</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
</pre></div>
</div>
<p>The area of each vertex is a function of the marginals information <span class="math notranslate nohighlight">\(H_1=I_1\)</span> and the thickness of the edges is a function of the pairwise
mutual information or total correlation <span class="math notranslate nohighlight">\(I_2=G_2\)</span>. On Iris dataset, it gives:</p>
<img alt="_images/iris_info_network.png" src="_images/iris_info_network.png" />
<p>The adjacency matrix of information have the marginals informations <span class="math notranslate nohighlight">\(H_1=I_1\)</span> in its diagonal and is symmetric with respect to the diagonal
as the result of the commutativity of the join-variables and mutual-variables operation in classical information theory (classical is by opposition
with quantum information theory). Compared to usual distance matrix (with given metric) computed in machine learning (for clustering or classifications),
the  <span class="math notranslate nohighlight">\(I_k\)</span> are not metric (e.g. non zero diagonal and no triangle inequality), we will introduce to information metric in the next stepps.
With such Matrix it is possible to apply some usual computational persistence homology tools like <a class="reference external" href="https://github.com/scikit-tda">Mapper scikit-tda</a>
(created by Singh, Mémoli, and Carlsson) and to build what could be called an “informational Vietoris-Ripps complex”. In the context of Morse theory,
information landscapes consider infomation functions themselfs as height or “Morse” functions. However there is likely a much more fundamental application of
persistence theory in the construction of a local probability density estimation (to be done). <span class="math notranslate nohighlight">\(I_k\)</span> with <span class="math notranslate nohighlight">\(k \geq 3\)</span> can be repesented in an
analgous way using k-cliques as acheived in <a class="reference external" href="https://www.nature.com/articles/s41598-018-31765-z">Tapia &amp; al 2018</a> (to be done in the package). They
shall be represented using k-tensor formalism. In the context of complex networks studies those higher <span class="math notranslate nohighlight">\(I_k\)</span> with <span class="math notranslate nohighlight">\(k \geq 3\)</span> correspond to
hypergraphs or <a class="reference external" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198753919.001.0001/oso-9780198753919">multiplex or multilayer networks</a>
The raw result obtained here is a fully connected network, but one can obtain a sparse matrix and a sparsely connected network by thresholding
the <span class="math notranslate nohighlight">\(I_k\)</span> with a with fixed p-value, using the exact statistical dependence test implemented in the package.</p>
<p>We begin to see that Homology provides a wide generalisation of complex networks (a 1-complex, that is a graph) to higher interactions structures.</p>
</div>
</div>
<div class="section" id="diabetes-data">
<h2>Diabetes data<a class="headerlink" href="#diabetes-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="diabetes-dataset">
<h3>Diabetes dataset<a class="headerlink" href="#diabetes-dataset" title="Permalink to this headline">¶</a></h3>
<p>The Iris dataset and its associated information landsacpes are in too low dimension to appreciate all the interest of the methods in higher dimensions,
so lets turn to larger dimensional classical machine learning dataset: Diabetes dataset. This dataset is kindly also furnished by scikitlearn, and we load it with the same methods as previously:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">dimension_max</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dimension_tot</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sample_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">nb_of_values</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">forward_computation_mode</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">work_on_transpose</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">supervised_mode</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">sampling_mode</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">deformed_probability_mode</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
<p>This dataset contains 10 variables-dimensions for a sample size (number of points) of 442 and a target (label) variable which quantifies diabetes
progress. The ten variables are [age, sex, body mass index, average blood pressure, T-Cells, low-density lipoproteins, high-density lipoproteins,
thyroid stimulating hormone, lamotrigine, blood sugar level] in this order.</p>
</div>
<div class="section" id="id4">
<h3>Entropy<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>As before, we execute:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Nentropie</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">simplicial_entropies_decomposition</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">information_topo</span><span class="o">.</span><span class="n">entropy_simplicial_lanscape</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">)</span>
<span class="n">information_topo</span> <span class="o">=</span> <span class="n">infotopo</span><span class="p">(</span><span class="n">dim_to_rank</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">number_of_max_val</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">dico_max</span><span class="p">,</span> <span class="n">dico_min</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_information</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>and we obtain the following entropy landscape:</p>
<img alt="_images/diabetes_entropy_landscape.png" src="_images/diabetes_entropy_landscape.png" />
<p>which corresponds to the following distributions of joint entropies for each dimensions:</p>
<img alt="_images/diabetes_entropy_histograms.png" src="_images/diabetes_entropy_histograms.png" />
<p>and the computation of the probability of encountering some undersampled probability density estimation (single point box) as a function of
the dimension gives:</p>
<img alt="_images/diabetes_undersampling.png" src="_images/diabetes_undersampling.png" />
<p>Which imposing an arbitrary confidence of P&gt;0.05 (default value of the “p_value_undersmapling” parametter), gives a undersampling dimension
<span class="math notranslate nohighlight">\(k_u=6\)</span>, meaning that with such level of confidence one should not interpret the landscapes and information estimations (whatever)
above the 5th dimension. This method is very basic and can (or shall) be improved in several ways, notably a strategy exploring undersampling
or information paths should provide more relevant methods, adapted to data structure (to be done).</p>
<p>The number of tuples (a total of <span class="math notranslate nohighlight">\(2^{10})\)</span>) to represent becomes to hudge, and enforces to plot only the distribution histograms of k-tuples
value (with a given number of bins = nb_bins_histo) in each dimension. We already see that there exist some interesting structures since the
distribution  of <span class="math notranslate nohighlight">\(H_3,H_4,H_5\)</span> display obvious bi-modality: the minimum joint entropy mode of the tuples contains the tuples the
furthest from randomness. The result shows for example that the 3 first minimum 4-entropy (figure below) contains the binary “sex” variable.
It points out one of the current possible limitation-bias of the present algorithm: for heterogeneous variable input, the algorithm should
allow different number of values adapted for each variable (binary ternary etc… at the moment their all the same… to be done).</p>
<img alt="_images/diabetes_3min_H4.png" src="_images/diabetes_3min_H4.png" />
</div>
<div class="section" id="total-correlation">
<h3>Total correlation<a class="headerlink" href="#total-correlation" title="Permalink to this headline">¶</a></h3>
<p>We can now focus on the statistical depencies and <span class="math notranslate nohighlight">\(G_k\)</span> and <span class="math notranslate nohighlight">\(I_k\)</span> structures, we will first compute the total correlation <span class="math notranslate nohighlight">\(G_k\)</span>,
by running as previously the commands:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ntotal_correlation</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">total_correlation_simplicial_lanscape</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">)</span>
<span class="n">dico_max</span><span class="p">,</span> <span class="n">dico_min</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_information</span><span class="p">(</span><span class="n">Ntotal_correlation</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>and we obtain the following <span class="math notranslate nohighlight">\(G_k\)</span> landscape:</p>
<img alt="_images/diabetes_total_correlation_landscape.png" src="_images/diabetes_total_correlation_landscape.png" />
<p>which corresponds to the following distributions of free energy <span class="math notranslate nohighlight">\(G_k\)</span> for each dimensions:</p>
<img alt="_images/diabetes_total_correlation_histograms.png" src="_images/diabetes_total_correlation_histograms.png" />
<p>The structure of dependences appears much richer, notably the landscape exhibits nice and clearcut bimodal distribution of free energy from
dimension 3 to dimension 8. The data points 4-subspace corresponding to the two first minima and maxima of <span class="math notranslate nohighlight">\(G_4\)</span> look like this :</p>
<img alt="_images/diabetes_min_max_G4.png" src="_images/diabetes_min_max_G4.png" />
<p>As expected the two <span class="math notranslate nohighlight">\(G_4\)</span> minima present the dependent 4-subspace, but the the two <span class="math notranslate nohighlight">\(G_4\)</span> maxima, for the 4-tuples (5,6,7,8)
and (5,6,8,9), present higly dependent very nice statistical dependencies (further detailed in the <span class="math notranslate nohighlight">\(I_4\)</span> subsection bellow).</p>
</div>
<div class="section" id="id5">
<h3>Mutual Information<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>We can now plot similarly the <span class="math notranslate nohighlight">\(I_k\)</span> landscape, using the commands:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ninfomut</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">simplicial_infomut_decomposition</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">)</span>
<span class="n">information_topo</span><span class="o">.</span><span class="n">mutual_info_simplicial_lanscape</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
<span class="n">dico_max</span><span class="p">,</span> <span class="n">dico_min</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_information</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<span class="n">adjacency_matrix_mut_info</span> <span class="o">=</span><span class="n">information_topo</span><span class="o">.</span><span class="n">mutual_info_pairwise_network</span><span class="p">(</span><span class="n">Ntotal_correlation</span><span class="p">)</span>
</pre></div>
</div>
<p>and we obtain the following <span class="math notranslate nohighlight">\(I_k\)</span> landscape:</p>
<img alt="_images/diabetes_information_landscape.png" src="_images/diabetes_information_landscape.png" />
<p>which corresponds to the following distributions of k-mutual information for each dimensions:</p>
<img alt="_images/diabetes_information_histograms.png" src="_images/diabetes_information_histograms.png" />
<p><span class="math notranslate nohighlight">\(I_k\)</span> landscape bring new results that could not be infered from total correlations, notably thanks to its possible negativity.
The <span class="math notranslate nohighlight">\(I_k\)</span> landscape of diabetes dataset notably displays important negative values (it was chosen to illustrate this very peculiar phenomena)
in dimension 3 and 4 for  some 3-tuples and 1 4-tuples (framed in blue). The data points 4-subspace corresponding to this minimal <span class="math notranslate nohighlight">\(I_4\)</span>
and the  maximal <span class="math notranslate nohighlight">\(I_4\)</span> look like this (with different views) :</p>
<img alt="_images/diabetes_min_max_I4.png" src="_images/diabetes_min_max_I4.png" />
<p>The tuple maximal <span class="math notranslate nohighlight">\(I_4\)</span> (framed in red) only display a weak correlation, as expected from the low <span class="math notranslate nohighlight">\(I_4\)</span> value. However the tuple with
minimal <span class="math notranslate nohighlight">\(I_4\)</span> (5,6,7,8) displays an impressive correlation structure taking the form of a 3 dimensional hyperplane (sligtly curved indeed).
Looking at projections on 2 dimensional subpaces as shown on the 3 plots on the right we see that the subspace corresponding to the tuples (5,6)
and (7,8) is higly “correlated” while  (6,7) and (5,7) are highly “random”. Indeed, the tuples (5,6), (7,8) and (6,8) obtain the maximum pairwise mutual
information. This phenomena of information negativity is known in neuroscience as synergy since the work of <a class="reference external" href="https://arxiv.org/abs/physics/9902067">Brenner et al</a>.
The fact that the 4-tuplet (5,6,7,8) have minimal and not maximal <span class="math notranslate nohighlight">\(I_4\)</span> provides us important additional information that cannot be deduced
form the pairwise <span class="math notranslate nohighlight">\(I_2\)</span> (e.g the fact that (5,6) and (7,8) have maximum <span class="math notranslate nohighlight">\(I_2\)</span>): the fact that the pair of variables  (5,6) and (7,8) and (6,8) untertain
causal relationship but have a common cause (another, possibly joint, variable). More precisely we can infer the following causal scheme:
<span class="math notranslate nohighlight">\(5 \rightarrow 6   \leftrightarrow 8  \leftarrow 7\)</span>  (with an ambiguity in the causal dierction between 6 and 8 that could be disambiguated by having
a look in the higher dimension 5, and an ambiguity in the global flow, all the arrows could be reversed, that could be desambiguated by looking at lower dimensions).
This is indeed equivalent to strong transfer entropy (or conditional mutual information, see <a class="reference external" href="https://arxiv.org/abs/nlin/0001042">Schreiber</a>) but applied here in a general
context without time series structure assumption. Transfer entropy is well known to generalize Granger causality to non-linear cases
(see <a class="reference external" href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.103.238701">Barnet et al</a>). The classical example of a common causal variable is
given   by: “as ice cream sales increase, the rate of drowning deaths increases sharply.”: both are correlated but none of each causes the other.
A section in “how_infotopo_works” is dedicated to a more complete study and explanation of these statistical interactions. The gene expression study
of <a class="reference external" href="https://www.nature.com/articles/s41598-018-31765-z">Tapia et al.</a> provides further examples of strong positive k-tuplet, e.g of statistical
interactions without common cause, or more simply causal chains (e.g metabolic chains).
The possiblity to extract causal relation from information structures, <span class="math notranslate nohighlight">\(I_k\)</span> landscape, is better illustrated by analysing the
<a class="reference external" href="http://www.causality.inf.ethz.ch/data/LUCAS.html">LUCAS0 Medical diagnosis dataset</a> sympathicaly proposed by the
<a class="reference external" href="http://www.causality.inf.ethz.ch/challenge.php?page=datasets">Causality Challenge #1: Causation and Prediction</a> . It can be acheived
by setting the variable dataset_type == 4 in the main of the python script after dowloading the csv at the previous link.
In this synthetic training example the 3 variables “smoking”, “genetics” and “lung cancer” (1,5,12) are among the minimal <span class="math notranslate nohighlight">\(I_3\)</span>
while they were designed  to exemplify the causal structure math:<cite>1 rightarrow 12 leftarrow 5</cite>. The dataset and causality results are detailed in the next section.</p>
</div>
<div class="section" id="id6">
<h3>Information Networks<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>The information networks representation of <span class="math notranslate nohighlight">\(I_1\)</span> and <span class="math notranslate nohighlight">\(I_2\)</span> for the diabetes dataset is:</p>
<img alt="_images/diabetes_information_networks.png" src="_images/diabetes_information_networks.png" />
<p>The maxima of <span class="math notranslate nohighlight">\(I_2\)</span> are for (5,6) then (7,8) then (6,8) and minima of <span class="math notranslate nohighlight">\(I_3\)</span> are for (5,7,8) then (6,7,8), and this indicate that 5 may
cause 7 and 8, and that 6 causes 7 and 8, while 5 and 6 are highly inter-dependent, among other relation, potentially complex relationships that
can be infered from the information landscape.</p>
</div>
<div class="section" id="mean-information-path">
<h3>Mean Information path<a class="headerlink" href="#mean-information-path" title="Permalink to this headline">¶</a></h3>
<p>It is interesting to compute and plot the mean <span class="math notranslate nohighlight">\(I_k\)</span> paths, which consist in dividing the sum of <span class="math notranslate nohighlight">\(I_k\)</span> by the binomial coefficient
<span class="math notranslate nohighlight">\(\binom{n}{k}\)</span>, and the Mean <span class="math notranslate nohighlight">\(I_k\)</span> rate , which consist in dividing the preceeding result by the dimension:</p>
<div class="math notranslate nohighlight">
\[\langle I_k \rangle = \frac{\sum_{T\subset [n];card(T)=i}I_k(X_T;P)}{\binom{n}{k}}\]</div>
<p>Using the command:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_info</span><span class="p">,</span> <span class="n">mean_info_rate</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_mean_information</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
</pre></div>
</div>
<p>we obtain the following  mean <span class="math notranslate nohighlight">\(I_k\)</span> paths and   mean <span class="math notranslate nohighlight">\(I_k\)</span> rate paths:</p>
<img alt="_images/diabetes_mean_Ik.png" src="_images/diabetes_mean_Ik.png" />
<p>Mean <span class="math notranslate nohighlight">\(I_k\)</span> corresponds to the mean-field approxiamtion in statistical physics, that assumes a homogeneous system with identical particles
and identical k-body interactions. We recover a usual free-energy landscape analogous to n-bdy van der Waals model, here with a (little) minima
at the critical dimension 3, which shows that the interactions (or statistical dependences) in the data are weak in average (almost the
independent case). The same computation and definitions can be acheived for k-entropy, and is let as an exercise.</p>
</div>
<div class="section" id="conditional-transfer-informations">
<h3>Conditional (transfer) Informations<a class="headerlink" href="#conditional-transfer-informations" title="Permalink to this headline">¶</a></h3>
<p>The visualization of information landscapes as histograms do not permit to visualize and study the conditional entropies and Mutual informations,
that can be very interesting as we saw with the (extension) of transfer entropy. They are given by chain rules and correspond to minus the slope
of each edges of the lattice in the landscapes. It is possible to plot them using the command:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">NcondInfo</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">conditional_info_simplicial_lanscape</span><span class="p">(</span><span class="n">Ninfomut</span><span class="p">)</span>
<span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_cond_information</span><span class="p">(</span><span class="n">NcondInfo</span><span class="p">)</span>
</pre></div>
</div>
<p>There are more conditional Informations than <span class="math notranslate nohighlight">\(I_k\)</span> (<span class="math notranslate nohighlight">\(k\binom{n}{k}\)</span> in each k-dimension, and <span class="math notranslate nohighlight">\(n2^{n-1}\)</span> in total), and we
encoded the output as a list for each dimension, “NcondInfo”, of dictionaries which items are of the forms ((5, 7, 9), 0.352)  for
the information of 5,7 knowing 9, e.g. I(5,7|9). Indeed, as remarked by (<a class="reference external" href="https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems">Han (1981)</a>
<a class="reference external" href="http://iest2.ie.cuhk.edu.hk/~whyeung/post/draft2.pdf">Yeung</a> generates
all the other information quantities we saw: considering the conditionning variable as the deterministic unit we obtain mutual informations, and
considering equivalent variables we obtain conditional entropies and entropies. Both the “Shannonian” and “non-shannonian” inequalities found by
Yeung translates directly in information landscapes as bounds on the slope paths (or topological cones), unraveling their homological nature
(see <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/881">PDF</a>). For the diabetes dataset, we obtain:</p>
<img alt="_images/diabetes_condinfo_landscape.png" src="_images/diabetes_condinfo_landscape.png" />
</div>
<div class="section" id="entropy-vs-energy">
<h3>Entropy Vs. Energy<a class="headerlink" href="#entropy-vs-energy" title="Permalink to this headline">¶</a></h3>
<p>Following the <a class="reference external" href="https://en.wikipedia.org/wiki/Gibbs_free_energy">original figure</a> ENTROPY vs. ENERGY vs. VOLUME of Willard Gibbs (1873) James
Clerk Maxwell (1874), we can resume part of the preceding results by ploting <span class="math notranslate nohighlight">\(H_k\)</span> (absyssa) vs. <span class="math notranslate nohighlight">\(G_k\)</span> (ordinate) using the command:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">information_topo</span><span class="o">.</span><span class="n">display_entropy_energy_landscape</span><span class="p">(</span><span class="n">Ntotal_correlation</span><span class="p">,</span> <span class="n">Nentropie</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/diabetes_entropy_energyGk_landscape.png" src="_images/diabetes_entropy_energyGk_landscape.png" />
<p>It notably shows how two population of data points clusters from dimension 6 to 8.</p>
</div>
<div class="section" id="information-distance">
<h3>Information distance<a class="headerlink" href="#information-distance" title="Permalink to this headline">¶</a></h3>
<p>Another nice information measure is information distance or metric defined by <span class="math notranslate nohighlight">\(V_2(X;Y) =H_2(X,Y)-I_2(X;Y)\)</span> . It is a “real” (and unique see <a class="reference external" href="https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems">Han for unicity proof</a>
metric in the sens that it satifies triangle inequalities and symmetry (precisely except identity if null, it is even better than a metric, it is a pseudo-metric). This metric was
find by  <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/1188572">Shannon (1953)</a> ,
and was the subject of further interesting both applied and theoretical studies (<a class="reference external" href="https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems">Han 1981</a> ,
<a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwj77P3Cp9DrAhUNExQKHfZSAxUQFjAAegQIBRAB&amp;url=https%3A%2F%2Fcore.ac.uk%2Fdownload%2Fpdf%2F82383189.pdf&amp;usg=AOvVaw2WmOW58ouwhVMBNifqz4ej">Rajski 1961</a>
, <a class="reference external" href="https://www.nature.com/articles/341119a0">Zurek</a> , <a class="reference external" href="https://arxiv.org/abs/1006.3520">Bennett</a> and
<a class="reference external" href="https://www.researchgate.net/publication/1773919_MIC_Mutual_Information_Based_Hierarchical_Clustering">Kraskov and Grassberger</a>).  It indeed appears as a
topological invariant in a precise setting  cohomological setting and generalises to the multivariate case to k information volumes
<span class="math notranslate nohighlight">\(V_k =H_k(X,Y)-I_k(X;Y)\)</span>  <a class="reference external" href="https://www.mdpi.com/1099-4300/21/9/881">PDF</a> .  <span class="math notranslate nohighlight">\(V_k\)</span> are non-neagtive and symmetric functions. For Machine Learning,
this shall be understood as an informational version of Jaccard metric, intersection over union (iou) or other union minus intersection metrics.
We can compute their simplicial structure using the commands:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ninfo_volume</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">information_volume_simplicial_lanscape</span><span class="p">(</span><span class="n">Nentropie</span><span class="p">,</span> <span class="n">Ninfomut</span><span class="p">)</span>
<span class="n">dico_max</span><span class="p">,</span> <span class="n">dico_min</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">display_higher_lower_information</span><span class="p">(</span><span class="n">Ninfo_volume</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<span class="n">adjacency_matrix_info_distance</span> <span class="o">=</span> <span class="n">information_topo</span><span class="o">.</span><span class="n">mutual_info_pairwise_network</span><span class="p">(</span><span class="n">Ninfo_volume</span><span class="p">)</span>
</pre></div>
</div>
<p>On the Diabete dataset, it gives the following <span class="math notranslate nohighlight">\(V_k\)</span> landscape:</p>
<img alt="_images/diabetes_info_volume_landscape.png" src="_images/diabetes_info_volume_landscape.png" />
<p>with the following distributions:</p>
<img alt="_images/diabetes_info_volume_histograms.png" src="_images/diabetes_info_volume_histograms.png" />
<p>We see that the structure is less interesting compared to the one we obtained with <span class="math notranslate nohighlight">\(I_k\)</span> and <span class="math notranslate nohighlight">\(G_k\)</span>, but its geometrical status of a
(pseudo)-metric leaves it appealing to plot in its network representation.</p>
<p>Beware that these tools will not detect whatever possible statistical dependencies (see James and Crutchfield <a class="reference external" href="https://www.mdpi.com/1099-4300/19/10/531">PDF</a>),
this is just a simplicial subset (nice…paths are automorphism) subsets, computationnally tractable. The complete structure of dependencies are spanned by general information structures and
lattice of patition (see section how_infotopo_works), which embedds the present simplicial case.
This concludes our introduction to basic infotopo usage – hopefully this
has given you the tools to get started for yourself. Further tutorials,
covering infotopo parameters and more advanced usage are also available when
you wish to dive deeper.(X)</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
    <hr>
    <footer>
        <br>
        <div class="container">
            <ul class="list-inline">
                <li style="font-weight:bold"><a href="genindex.html">Index</a></li>
                <li>·</li>
                <li style="font-weight:bold"><a href="https://github.com/pierrebaudot/infotopopy">Github</a></li>
                <li>·</li>
                <li style="font-weight:bold"><a href="references.html">References</a></li>
            </ul>
         </div>
        <!-- <br> -->
        <p>&copy; Copyright 2020, Pierre Baudot.</p>
    </footer>

  </body>
</html>